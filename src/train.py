import pandas as pd
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.impute import SimpleImputer
import numpy as np

def evaluate_model(y_true, y_pred, y_proba):
    return {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'roc_auc': roc_auc_score(y_true, y_proba)
    }

def train_models(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Impute any remaining NaN values
    imputer = SimpleImputer(strategy='median')
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)
    
    # Logistic Regression
    with mlflow.start_run(run_name="logistic_regression"):
        lr = LogisticRegression()
        param_grid = {'C': [0.1, 1, 10], 'penalty': ['l2']}
        grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')
        grid_search.fit(X_train, y_train)
        
        # Evaluate and log
        y_pred = grid_search.predict(X_test)
        y_proba = grid_search.predict_proba(X_test)[:, 1]
        metrics = evaluate_model(y_test, y_pred, y_proba)
        mlflow.log_params(grid_search.best_params_)
        mlflow.log_metrics(metrics)
        mlflow.sklearn.log_model(grid_search.best_estimator_, "logistic_regression")
    
    # Gradient Boosting
    with mlflow.start_run(run_name="gradient_boosting"):
        gb = GradientBoostingClassifier()
        param_dist = {
            'n_estimators': [100, 200],
            'learning_rate': [0.01, 0.1],
            'max_depth': [3, 5]
        }
        random_search = RandomizedSearchCV(gb, param_dist, n_iter=10, cv=5, scoring='roc_auc')
        random_search.fit(X_train, y_train)
        
        # Evaluate and log
        y_pred = random_search.predict(X_test)
        y_proba = random_search.predict_proba(X_test)[:, 1]
        metrics = evaluate_model(y_test, y_pred, y_proba)
        mlflow.log_params(random_search.best_params_)
        mlflow.log_metrics(metrics)
        mlflow.sklearn.log_model(random_search.best_estimator_, "gradient_boosting")

if __name__ == "__main__":
    df = pd.read_csv('../data/processed/processed_data_with_features.csv')
    
    # Check for is_high_risk column
    if 'is_high_risk' not in df.columns:
        raise KeyError("Column 'is_high_risk' not found in processed_data_with_features.csv. Available columns: " + str(list(df.columns)))
    
    # Define features to keep (processed numerical and categorical columns)
    feature_cols = ['TotalAmount', 'AvgAmount', 'TransactionCount', 'StdAmount', 'TransactionHour']
    # Include one-hot-encoded columns (dynamically generated by ProductCategory and ChannelId)
    feature_cols += [col for col in df.columns if col.startswith('ProductCategory_') or col.startswith('ChannelId_')]
    
    # Create X and y
    X = df[feature_cols]
    y = df['is_high_risk']
    
    # Check for NaN values
    if X.isna().any().any():
        print("Warning: NaN values found in X. Columns with NaN:", X.columns[X.isna().any()].tolist())
    
    train_models(X, y)